{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit: Xuan Lei (Lei.337@osu.edu)\n",
    "\n",
    "## This demo provides an implementation of surface coil intensity correction (SCC) for data collected on a Siemens scanner.\n",
    "### Make sure that you have cloned the [SCC repository](https://github.com/OSU-MR/SCC) from GitHub and are executing brightness_correction_demo_ipynb from the SCC folder.\n",
    "### Before you run the demo, make sure that the your environment is setup properly. To do that, execute these commands in the terminal. Then, make sure that the SCC environment is selected for this code to run.\n",
    "* conda create --name SCC python=3.8\n",
    "* conda activate SCC\n",
    "* conda install jupyterlab\n",
    "* conda install ipykernel\n",
    "* python -m ipykernel install --user --name=SCC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) The following cell will install [Twix Tools](https://github.com/OSU-MR/Python_read_Siemens_rawdata) and then replace twix_map.py with the one modified by Dr. Chong Chen (GitHub ID: MRIOSU)\n",
    "* Twix Tools are needed to read the raw datafile from Siemens scanners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please read README.md to run the code.\n",
      "The code is still under development.\n",
      "plesae contact me for adding more data shape support.\n",
      "lei.337@osu.edu\n",
      "Downloading twixtools...\n",
      "Extracting the zip file...\n",
      "Replacing twix_map.py...\n",
      "Sufficient numpy version is already installed.\n",
      "Installing twixtools...\n",
      "Processing /Users/ahmad.46/Library/CloudStorage/OneDrive-TheOhioStateUniversity/software/github-repos/SCC/twixtools-master\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from twixtools==1.0) (1.23.4)\n",
      "Requirement already satisfied: scipy in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from twixtools==1.0) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from twixtools==1.0) (4.66.1)\n",
      "Building wheels for collected packages: twixtools\n",
      "  Building wheel for twixtools (setup.py): started\n",
      "  Building wheel for twixtools (setup.py): finished with status 'done'\n",
      "  Created wheel for twixtools: filename=twixtools-1.0-py3-none-any.whl size=51389 sha256=1cc523b695dbd142d7a1cf8037f87d64eb23d020a01e18a90a1fda1be3490e1a\n",
      "  Stored in directory: /Users/ahmad.46/Library/Caches/pip/wheels/46/e8/b9/a19a7d980fc65bd8322e2a43100810615748a8b7cdc5061903\n",
      "Successfully built twixtools\n",
      "Installing collected packages: twixtools\n",
      "  Attempting uninstall: twixtools\n",
      "    Found existing installation: twixtools 1.0\n",
      "    Uninstalling twixtools-1.0:\n",
      "      Successfully uninstalled twixtools-1.0\n",
      "Successfully installed twixtools-1.0\n",
      "Removing the zip file and the extracted directory...\n",
      "Installing numpy==1.23.4...\n",
      "Requirement already satisfied: numpy==1.23.4 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (1.23.4)\n",
      "Installing sigpy==0.1.25...\n",
      "Requirement already satisfied: sigpy==0.1.25 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (0.1.25)\n",
      "Requirement already satisfied: numpy in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from sigpy==0.1.25) (1.23.4)\n",
      "Requirement already satisfied: pywavelets in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from sigpy==0.1.25) (1.4.1)\n",
      "Requirement already satisfied: numba in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from sigpy==0.1.25) (0.57.1)\n",
      "Requirement already satisfied: scipy in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from sigpy==0.1.25) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from sigpy==0.1.25) (4.66.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from numba->sigpy==0.1.25) (0.40.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from numba->sigpy==0.1.25) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from importlib-metadata->numba->sigpy==0.1.25) (3.11.0)\n",
      "Installing matplotlib==3.7.2...\n",
      "Requirement already satisfied: matplotlib==3.7.2 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from matplotlib==3.7.2) (6.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.2) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ahmad.46/anaconda3/envs/SCC/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2) (1.16.0)\n",
      "Installation of missing packages complete!\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "from helper_functions.download_data import install_twixtools \n",
    "install_twixtools()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- # 2. Organize the folders of your datasets\n",
    "# **The sturcture of your dataset folders should be like this:**\n",
    "<style>\n",
    "code {\n",
    "    font-size: 15px;\n",
    "}\n",
    "</style>\n",
    "```markdown\n",
    "\n",
    "\n",
    "base_dir-----input_folder-----folder name of your datasets_1\n",
    "          |                |--folder name of your datasets_2\n",
    "          |                |              ...\n",
    "          |\t\t        --folder name of your datasets_n\n",
    "          |\n",
    "          |\n",
    "          | (folders below will be automatically created)\n",
    "          ---output_folder------correction map folder of your datasets_1    \n",
    "\t\t\t     |--correction map folder of your datasets_2         \n",
    "\t\t\t     |                ...                 \n",
    "                 ---correction map folder of your datasets_n\n",
    "\n",
    "``` -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) This cell will automatically download a raw data file that we have uploaded on [figshare](figshare.com). Be patient, downloading the file may take a few minutes.\n",
    "* The downloaded file is saved in SCC/data/rawdata/demo folder while the results are saved on SCC/data/results/demo\n",
    "* The link 'url_2CH' has a series of images from a single slice (620 MB). The link 'url_SAX' has series of images from multiple slices (5.2 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo datasets are being downloaded, please wait......\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from helper_functions.download_data import download_file_from_figshare\n",
    "# Path: ./data\n",
    "#create base directory\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "#create subdirectories\n",
    "os.makedirs('./data/rawdata', exist_ok=True)\n",
    "os.makedirs('./data/rawdata/demo', exist_ok=True)\n",
    "\n",
    "#download data\n",
    "saving_path = './data/rawdata/demo/'\n",
    "url_2CH = \"https://figshare.com/ndownloader/files/41881158\" # image collected in two-chambers view\n",
    "url_SAX = \"https://figshare.com/ndownloader/files/41915115\" # image stack collected in short-axis view\n",
    "\n",
    "print(\"Demo datasets are being downloaded, please wait......\")\n",
    "download_file_from_figshare(saving_path, url_2CH) # May take a couple of minutes to download\n",
    "#download_file_from_figshare(saving_path,url_SAX) # May take tens of minutes to download\n",
    "\n",
    "base_dir = \"./data\"\n",
    "input_folder = \"rawdata\"\n",
    "subfolder = ['demo'] #set this to None, if you want to go through all the subfolders inside input_folder\n",
    "output_folder = \"results\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # 3. Import the functions and define the folders -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Importing functions and setting the input/output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from helper_functions.brightness_correction import , getting_and_saving_correction_map, create_and_start_threadings, \n",
    "from helper_functions.brightness_correction import target_path_generator, displaying_results, rawdata_reader, correction_map_generator, auto_image_rotation, data_reduction, save_sense_recon_results\n",
    "from helper_functions.recon import sense_reconstruction, remove_edges, rotate_image, pad_ref\n",
    "from helper_functions.preprocess import ifftnd, rms_comb,remove_RO_oversamling, compress_data_with_pca\n",
    "\n",
    "path_input, path_output,  = target_path_generator(base_dir, input_folder, output_folder, subfolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Option 1: Applying the SCC method to correct reconstruced images\n",
    "#### img_correction_map is used to correct reconstructed images by element-wise multiplying img_correction_map with the reconstructed coil-combined image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rotation = 'LGE' # 'Dicom' or 'LGE'\n",
    "apply_correction_to_sensitivity_maps = False # True or False\n",
    "channel_keep = 8 # number of channels to keep after coil compression if you don't want to do coil compression then set this to None\n",
    "lamb = 1e0      # regularization parameter for generating the correction maps, the larger the value the smoother the maps\n",
    "\n",
    "for i, data_path_filename in enumerate(path_input):\n",
    "    \n",
    "    twix, image_3D_body_coils, image_3D_surface_coils, data, dim_info_data, data_ref, dim_info_ref, num_sli = rawdata_reader(data_path_filename)\n",
    "\n",
    "    #coil compression with PCA, you can use the u parameter to preform the same coil compression on extra data\n",
    "    data, data_ref, image_3D_surface_coils, u = compress_data_with_pca(data = data, channel_keep = channel_keep, axis_CH= dim_info_data.index('Cha'), \n",
    "                             extra_data=data_ref, extra_axis_CH=dim_info_ref.index('Cha'), \n",
    "                             extra_data2=image_3D_surface_coils, extra_axis_CH2=-1)\n",
    "\n",
    "    #coil compression end\n",
    "\n",
    "    img_correction_map, sens_correction_map, low_resolution_surface_coil_imgs, img_quats = correction_map_generator(twix, image_3D_body_coils, image_3D_surface_coils, data, dim_info_data, num_sli , auto_rotation = auto_rotation, lamb = lamb)\n",
    "\n",
    "    #define sense recon results\n",
    "    sense_recon_results = np.zeros((num_sli,data.shape[dim_info_data.index('Lin')],data.shape[dim_info_data.index('Col')]//2),dtype=np.complex64)\n",
    "    \n",
    "    #only do recon one image for each different slices (this is for demo purpose)\n",
    "    data,dim_info_data = data_reduction(data,data_dimensions = dim_info_data, dims_to_keep = ['Sli', 'Lin', 'Cha', 'Col'])\n",
    "\n",
    "    for n in range(num_sli):\n",
    "        #preprocessing for the sense reconstruction\n",
    "        ksp,ref_padded = pad_ref(data,data_ref,n,dim_info_ref = dim_info_ref,dim_info_org=dim_info_data)\n",
    "        ksp = remove_RO_oversamling(ksp,axis_RO=dim_info_data.index('Col'))\n",
    "        ref_padded = remove_RO_oversamling(ref_padded,axis_RO=dim_info_ref.index('Col'))\n",
    "\n",
    "        if apply_correction_to_sensitivity_maps:\n",
    "            #if we want to apply the correction during the sense reconstruction\n",
    "            sense_reconstructed_img = sense_reconstruction(ksp,ref_padded,sens_correction_map[n,...])\n",
    "        else:\n",
    "            sense_reconstructed_img = sense_reconstruction(ksp,ref_padded)\n",
    "\n",
    "        sense_recon_results[n,...] = sense_reconstructed_img\n",
    "\n",
    "\n",
    "    #we are rotating the images here (pptional, may not apply to other datasets)\n",
    "    sense_recon_results, img_correction_map, sens_correction_map = auto_image_rotation(sense_recon_results, img_quats, \n",
    "                                                                            auto_rotation = auto_rotation, \n",
    "                                                                            img_correction_map = img_correction_map, \n",
    "                                                                            sens_correction_map = sens_correction_map,\n",
    "                                                                            filename = data_path_filename)\n",
    "    \n",
    "    #save the results\n",
    "    save_sense_recon_results(path_output[i],sense_recon_results, img_correction_map, sens_correction_map, img_quats, apply_correction_to_sensitivity_maps)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Option 2: Applying the SCC method to correct sensitivity maps\n",
    "#### senitivity_correction_maps is used to correct ESPIRiT maps by element-wise multiplying ESPIRiT maps with a with sensivity_correction_map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auto_rotation = 'LGE' # 'Dicom' or 'LGE'\n",
    "apply_correction_to_sensitivity_maps = True # True or False\n",
    "channel_keep = 8 # number of channels to keep after coil compression if you don't want to do coil compression then set this to None\n",
    "lamb = 1e0      # regularization parameter for generating the correction maps the larger the value the more smoothing\n",
    "\n",
    "for i, data_path_filename in enumerate(path_input):\n",
    "    \n",
    "    twix, image_3D_body_coils, image_3D_surface_coils, data, dim_info_data, data_ref, dim_info_ref, num_sli = rawdata_reader(data_path_filename)\n",
    "\n",
    "    #coil compression with PCA, you can use the u parameter to preform the same coil compression on extra data\n",
    "    data, data_ref, image_3D_surface_coils, u = compress_data_with_pca(data = data, channel_keep = channel_keep, axis_CH= dim_info_data.index('Cha'), \n",
    "                             extra_data=data_ref, extra_axis_CH=dim_info_ref.index('Cha'), \n",
    "                             extra_data2=image_3D_surface_coils, extra_axis_CH2=-1)\n",
    "\n",
    "    #coil compression end\n",
    "\n",
    "    img_correction_map, sens_correction_map, low_resolution_surface_coil_imgs, img_quats = correction_map_generator(twix, image_3D_body_coils, image_3D_surface_coils, data, dim_info_data, num_sli , auto_rotation = auto_rotation, lamb = lamb)\n",
    "\n",
    "    #define sense recon results\n",
    "    sense_recon_results = np.zeros((num_sli,data.shape[dim_info_data.index('Lin')],data.shape[dim_info_data.index('Col')]//2),dtype=np.complex64)\n",
    "    \n",
    "    #only do recon one image for each different slices (this is for demo purpose)\n",
    "    data,dim_info_data = data_reduction(data,data_dimensions = dim_info_data, dims_to_keep = ['Sli', 'Lin', 'Cha', 'Col'])\n",
    "\n",
    "    for n in range(num_sli):\n",
    "        #preprocessing for the sense reconstruction\n",
    "        ksp,ref_padded = pad_ref(data,data_ref,n,dim_info_ref = dim_info_ref,dim_info_org=dim_info_data)\n",
    "        ksp = remove_RO_oversamling(ksp,axis_RO=dim_info_data.index('Col'))\n",
    "        ref_padded = remove_RO_oversamling(ref_padded,axis_RO=dim_info_ref.index('Col'))\n",
    "\n",
    "        if apply_correction_to_sensitivity_maps:\n",
    "            #if we want to apply the correction during the sense reconstruction\n",
    "            sense_reconstructed_img = sense_reconstruction(ksp,ref_padded,sens_correction_map[n,...])\n",
    "        else:\n",
    "            sense_reconstructed_img = sense_reconstruction(ksp,ref_padded)\n",
    "\n",
    "        sense_recon_results[n,...] = sense_reconstructed_img\n",
    "\n",
    "\n",
    "    #we are rotating the images here (pptional, may not apply to other datasets)\n",
    "    sense_recon_results, img_correction_map, sens_correction_map = auto_image_rotation(sense_recon_results, img_quats, \n",
    "                                                                            auto_rotation = auto_rotation, \n",
    "                                                                            img_correction_map = img_correction_map, \n",
    "                                                                            sens_correction_map = sens_correction_map,\n",
    "                                                                            filename = data_path_filename)\n",
    "    \n",
    "    #save the results\n",
    "    save_sense_recon_results(path_output[i],sense_recon_results, img_correction_map, sens_correction_map, img_quats, apply_correction_to_sensitivity_maps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Displaying results from an example slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sli_idx = 0 # changing this number will show different slices\n",
    "subfolder = ['demo'] #set this to None, if you want to go through all the subfolders inside input_folder\n",
    "output_folder = \"results\"\n",
    "displaying_results(base_dir=base_dir, input_folder=input_folder,\n",
    "                   output_folder=output_folder, folder_names=subfolder, sli_idx=sli_idx,\n",
    "                   fig_h=9, show_both=True) #set show_both = True to display both results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "4841b938622dfe38e2e2b1649ec57ba0537f88bb5a7138833f707105bc904c38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
